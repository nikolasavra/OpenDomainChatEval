{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c0ced28-e501-4702-b957-be72f8dcac90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.6.1 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71991fc9-95a9-493f-a695-9144383d2a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.10.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch)\n",
      "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (69.1.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:09\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m943.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0mm\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[33m  WARNING: The scripts proton and proton-viewer are installed in '/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script isympy is installed in '/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts torchfrtrace and torchrun are installed in '/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed filelock-3.18.0 fsspec-2025.5.1 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 triton-3.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install torch\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8cd853b5-c890-48e7-940e-8605529a063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'my_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "442d2583-67f2-4b5f-8e9b-4fba6a61e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "eb2b2012-5d4c-474a-bcfc-2afc187ab20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Label  \\\n",
      "0  adversarial   \n",
      "1  adversarial   \n",
      "2  adversarial   \n",
      "3     original   \n",
      "4     original   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                        Dialogue  \\\n",
      "0  What made you think that she wasn't very confident ? Did you notice the way that she avoided making eye contact with us while she talked ? She was a bit nervous , I guess . What else ? When she first walked into the room to greet us , she didn't shake our hands or introduce herself at all . I thought that was a bit unprofessional .   \n",
      "1                                                                                                                                                                                                                  Yes . She goes three times a week . Why ? What does she want to be ? She wants to be a manager . What classes does she take ?   \n",
      "2                                                                                                                                                                                                                                                                                                                                  What ’ s up ?   \n",
      "3                                                                                                  well , the free stuff ! you know the soap , lotion , shampoo , etc . i see . sir , if i may ask . have you used up all the amenities in your room ? not at all . i still have enough for the next few days . then what is the problem , sir ?   \n",
      "4                                           As an experienced interviewer, could you tell me what kind of behavior the interviewer hates most? Rudeness . You are not expected to sit down before being invited, and you shouldn ’ t throw yourself into the chair like you ’ re loading heavy baggage. And we should have good sitting posture.   \n",
      "\n",
      "                                                                                                                                                                             Response  \n",
      "0                                                               its fun talking about different things without putting on pants. if i see a steak my wife will probably say hamburger  \n",
      "1  After a week , you ’ ll be running around here like a pro . Let me give you this list of departments first , next to each department is its location and the name of the manager .  \n",
      "2                                                                                                                                         taking a break from homework what about you  \n",
      "3                                                                                                                                                  i need some to keep as souvenirs !  \n",
      "4                                                                                                                          Arrogance is another behavior which the interviewer hates.  \n"
     ]
    }
   ],
   "source": [
    "# Load and process only the first 100,000 lines\n",
    "data = []\n",
    "with open('train_dailydialog.txt', 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= 400000:\n",
    "            break\n",
    "        parts = line.strip().split('|||')\n",
    "        if len(parts) == 3:\n",
    "            label = parts[0].strip()\n",
    "            dialogue = parts[1].strip()\n",
    "            response = parts[2].strip()\n",
    "            data.append([label, dialogue, response])\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(data, columns=['Label', 'Dialogue', 'Response'])\n",
    "pd.options.display.max_colwidth = 400  # Adjust this value as needed\n",
    "\n",
    "# Check the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1a376508-b37e-41c7-a591-953e122cd86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Label  \\\n",
      "0      1   \n",
      "1      1   \n",
      "2      1   \n",
      "3      0   \n",
      "4      0   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                        Dialogue  \\\n",
      "0  What made you think that she wasn't very confident ? Did you notice the way that she avoided making eye contact with us while she talked ? She was a bit nervous , I guess . What else ? When she first walked into the room to greet us , she didn't shake our hands or introduce herself at all . I thought that was a bit unprofessional .   \n",
      "1                                                                                                                                                                                                                  Yes . She goes three times a week . Why ? What does she want to be ? She wants to be a manager . What classes does she take ?   \n",
      "2                                                                                                                                                                                                                                                                                                                                  What ’ s up ?   \n",
      "3                                                                                                  well , the free stuff ! you know the soap , lotion , shampoo , etc . i see . sir , if i may ask . have you used up all the amenities in your room ? not at all . i still have enough for the next few days . then what is the problem , sir ?   \n",
      "4                                           As an experienced interviewer, could you tell me what kind of behavior the interviewer hates most? Rudeness . You are not expected to sit down before being invited, and you shouldn ’ t throw yourself into the chair like you ’ re loading heavy baggage. And we should have good sitting posture.   \n",
      "\n",
      "                                                                                                                                                                             Response  \n",
      "0                                                               its fun talking about different things without putting on pants. if i see a steak my wife will probably say hamburger  \n",
      "1  After a week , you ’ ll be running around here like a pro . Let me give you this list of departments first , next to each department is its location and the name of the manager .  \n",
      "2                                                                                                                                         taking a break from homework what about you  \n",
      "3                                                                                                                                                  i need some to keep as souvenirs !  \n",
      "4                                                                                                                          Arrogance is another behavior which the interviewer hates.  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Map string labels to integers\n",
    "label_map = {\n",
    "    'original': 0,\n",
    "    'adversarial': 1,\n",
    "    'random': 2\n",
    "}\n",
    "\n",
    "# Apply the mapping to the 'Label' column\n",
    "df['Label'] = df['Label'].map(label_map)\n",
    "\n",
    "# Ensure the Label column is in integer format\n",
    "df['Label'] = df['Label'].astype(int)\n",
    "\n",
    "# Check the DataFrame to confirm the label mapping\n",
    "print(df[['Label', 'Dialogue', 'Response']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f8181783-db1f-4bc7-b415-e1f61c4e237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution:\n",
      "Label\n",
      "2    133488\n",
      "0    133362\n",
      "1    133150\n",
      "Name: count, dtype: int64\n",
      "Label range: 0 to 2\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, AutoTokenizer\n",
    "\n",
    "# Use AutoTokenizer for better compatibility\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased', cache_dir='my_cache')\n",
    "\n",
    "def tokenize_function(row):\n",
    "    # Combine dialogue and response with [SEP] token for better separation\n",
    "    combined_text = f\"{row['Dialogue']} [SEP] {row['Response']}\"\n",
    "    \n",
    "    # Tokenize the combined text\n",
    "    tokenized = tokenizer(\n",
    "        combined_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,  # Specify max_length explicitly\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        'input_ids': tokenized['input_ids'].squeeze(),\n",
    "        'attention_mask': tokenized['attention_mask'].squeeze()\n",
    "    }\n",
    "\n",
    "# Apply tokenization to the DataFrame\n",
    "df['Tokenized'] = df.apply(tokenize_function, axis=1)\n",
    "\n",
    "# Extract input_ids and attention_mask into separate columns\n",
    "df['input_ids'] = df['Tokenized'].apply(lambda x: x['input_ids'].tolist())\n",
    "df['attention_mask'] = df['Tokenized'].apply(lambda x: x['attention_mask'].tolist())\n",
    "\n",
    "# Drop the Tokenized column\n",
    "df = df.drop(columns=['Tokenized'])\n",
    "\n",
    "# IMPORTANT: Ensure labels are in the correct range [0, 1, 2] for 3-class classification\n",
    "print(\"Label distribution:\")\n",
    "print(df['Label'].value_counts())\n",
    "print(f\"Label range: {df['Label'].min()} to {df['Label'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "95e53f63-aaf4-4b78-a523-716ab8417884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 320000\n",
      "Validation set size: 80000\n",
      "Training label distribution:\n",
      "Label\n",
      "2    106790\n",
      "0    106690\n",
      "1    106520\n",
      "Name: count, dtype: int64\n",
      "Validation label distribution:\n",
      "Label\n",
      "2    26698\n",
      "0    26672\n",
      "1    26630\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the data with stratification to maintain label distribution\n",
    "train_df, val_df = train_test_split(\n",
    "    df, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['Label']  # Ensure balanced splits\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(\"Training label distribution:\")\n",
    "print(train_df['Label'].value_counts())\n",
    "print(\"Validation label distribution:\")\n",
    "print(val_df['Label'].value_counts())\n",
    "\n",
    "# Reset indices after splitting\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.data['input_ids'].iloc[idx], dtype=torch.long)\n",
    "        attention_mask = torch.tensor(self.data['attention_mask'].iloc[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.data['Label'].iloc[idx], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = DialogueDataset(train_df)\n",
    "val_dataset = DialogueDataset(val_df)\n",
    "\n",
    "# Use smaller batch size and add drop_last to avoid issues with small batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "95540844-091b-4c82-acd9-1e7ecc16c293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=3,  # Specify number of classes\n",
    "    cache_dir='my_cache'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1804ae0b-964f-44f6-b119-ff132da7cf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2393ca59-e666-47e9-b350-500286b4b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            # Store for detailed analysis\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Print detailed validation info\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Predictions distribution: {np.bincount(all_preds)}\")\n",
    "    print(f\"True labels distribution: {np.bincount(all_labels)}\")\n",
    "    \n",
    "    return avg_val_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e9c084c3-70c3-4ddb-8cbe-ccb88b07e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, device, num_epochs=3):\n",
    "    # Learning rate scheduler\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=int(0.1 * total_steps),  # 10% warmup\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            # Move data to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track training metrics\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "            })\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}:\")\n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_accuracy = validate(model, val_loader, device)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Initialize optimizer with proper learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24311f62-2dae-4196-a480-7e84f0635ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:27:12<00:00,  7.64it/s, loss=0.2478, lr=1.67e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/12:\n",
      "Training Loss: 0.6016, Training Accuracy: 0.7342\n",
      "Validation Loss: 0.4274, Accuracy: 0.8275\n",
      "Predictions distribution: [27712 24584 27704]\n",
      "True labels distribution: [26672 26630 26698]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:27:12<00:00,  7.64it/s, loss=0.2656, lr=1.85e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/12:\n",
      "Training Loss: 0.4162, Training Accuracy: 0.8394\n",
      "Validation Loss: 0.4060, Accuracy: 0.8460\n",
      "Predictions distribution: [26633 28315 25052]\n",
      "True labels distribution: [26672 26630 26698]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:27:07<00:00,  7.65it/s, loss=0.0471, lr=1.67e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/12:\n",
      "Training Loss: 0.3383, Training Accuracy: 0.8905\n",
      "Validation Loss: 0.4556, Accuracy: 0.8504\n",
      "Predictions distribution: [29635 25725 24640]\n",
      "True labels distribution: [26672 26630 26698]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:27:08<00:00,  7.65it/s, loss=0.3649, lr=1.48e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/12:\n",
      "Training Loss: 0.2794, Training Accuracy: 0.9243\n",
      "Validation Loss: 0.5778, Accuracy: 0.8626\n",
      "Predictions distribution: [26664 26879 26457]\n",
      "True labels distribution: [26672 26630 26698]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:27:07<00:00,  7.65it/s, loss=0.0018, lr=1.30e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/12:\n",
      "Training Loss: 0.2159, Training Accuracy: 0.9481\n",
      "Validation Loss: 0.7219, Accuracy: 0.8575\n",
      "Predictions distribution: [28393 23435 28172]\n",
      "True labels distribution: [26672 26630 26698]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:27:11<00:00,  7.65it/s, loss=0.0008, lr=1.11e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/12:\n",
      "Training Loss: 0.1596, Training Accuracy: 0.9643\n",
      "Validation Loss: 0.8539, Accuracy: 0.8621\n",
      "Predictions distribution: [28328 25182 26490]\n",
      "True labels distribution: [26672 26630 26698]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:27:15<00:00,  7.64it/s, loss=0.0003, lr=9.26e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/12:\n",
      "Training Loss: 0.1150, Training Accuracy: 0.9760\n",
      "Validation Loss: 0.8956, Accuracy: 0.8642\n",
      "Predictions distribution: [26759 26071 27170]\n",
      "True labels distribution: [26672 26630 26698]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:27:14<00:00,  7.64it/s, loss=0.0003, lr=7.41e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/12:\n",
      "Training Loss: 0.0806, Training Accuracy: 0.9839\n",
      "Validation Loss: 0.9948, Accuracy: 0.8651\n",
      "Predictions distribution: [28146 25618 26236]\n",
      "True labels distribution: [26672 26630 26698]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/12: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:27:14<00:00,  7.64it/s, loss=0.0000, lr=5.56e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/12:\n",
      "Training Loss: 0.0573, Training Accuracy: 0.9887\n",
      "Validation Loss: 1.0140, Accuracy: 0.8687\n",
      "Predictions distribution: [27238 26515 26247]\n",
      "True labels distribution: [26672 26630 26698]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/12: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 40000/40000 [1:30:36<00:00,  7.36it/s, loss=0.0000, lr=3.70e-06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/12:\n",
      "Training Loss: 0.0390, Training Accuracy: 0.9926\n",
      "Validation Loss: 1.0813, Accuracy: 0.8666\n",
      "Predictions distribution: [28645 25575 25780]\n",
      "True labels distribution: [26672 26630 26698]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/12:  54%|███████████████████████████████████████████████████████████████████████████████████                                                                        | 21435/40000 [46:44<40:29,  7.64it/s, loss=0.0001, lr=2.71e-06]"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "train(model, train_loader, val_loader, optimizer, device, num_epochs=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a1a1517d-26c3-46ae-bb30-915f0bb419e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Save the complete model (RECOMMENDED)\n",
    "# This saves both the model architecture and weights\n",
    "def save_complete_model(model, tokenizer, save_directory=\"./saved_model\"):\n",
    "    \"\"\"\n",
    "    Save the complete model and tokenizer for easy reuse\n",
    "    \"\"\"\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    \n",
    "    # Save model and tokenizer (HuggingFace format)\n",
    "    model.save_pretrained(save_directory)\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    \n",
    "    print(f\"Model and tokenizer saved to: {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "57fd8b78-1a50-4e20-8ef1-fca5197d5fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to: ./my_dialogue_classifier_12_400\n"
     ]
    }
   ],
   "source": [
    "save_complete_model(model, tokenizer, \"./my_dialogue_classifier_12_400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785f74b-5915-43c9-b3d9-8f7e13083ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
