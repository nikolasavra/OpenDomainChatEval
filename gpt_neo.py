# -*- coding: utf-8 -*-
"""GPT-Neo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VPJdzlnFM20u2IJVLqhLkxWKIHb9aQB1
"""

import pandas as pd
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Load GPT-Neo model and tokenizer
model_name = "EleutherAI/gpt-neo-1.3B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

import torch

if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu")
    print("Using CPU")
model.to(device)

# Load the CSV file
dataset_path = "chit_chat.csv"
df = pd.read_csv(dataset_path)

# Function to generate responses using GPT-Neo
def generate_response_gptneo(prompt, max_length=150):
    # Tokenize the input and get only 'input_ids'
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    input_ids = inputs['input_ids'].to(device)

    # Generate response
    outputs = model.generate(
        input_ids,
        max_length=max_length,
        num_return_sequences=1,
        temperature=0.8,        # Adjust for more creativity
        top_k=50,               # Top-k sampling
        top_p=0.85,             # Nucleus sampling
        repetition_penalty=1.2,  # Increase penalty to avoid repetition
        do_sample=True
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return response

user_prompts = df['content']

# List to store results
gptneo_responses = []

# Loop through each user prompt and generate a response
total_prompts = len(user_prompts)
for idx, prompt in enumerate(user_prompts):
    response = generate_response_gptneo(prompt)
    gptneo_responses.append(response)

    # Print progress
    print(f"Generated response {idx + 1}/{total_prompts} ({total_prompts - (idx + 1)} remaining)")

    # Print the first 10 responses to check everything runs well
    if idx < 10:
        print(f"Prompt {idx + 1}: {prompt}")
        print(f"Response {idx + 1}: {response}")
        print("-" * 50)  # Separator for clarity

# Save the prompts and responses to a CSV file
evaluation_data_gptneo = pd.DataFrame({
    'Prompt': user_prompts,
    'Response': gptneo_responses
})

evaluation_data_gptneo.to_csv('gptneo_responses.csv', index = False)