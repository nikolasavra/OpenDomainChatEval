# -*- coding: utf-8 -*-
"""Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZX8UGqgQQWNKoKE-KJanpi1Up_P1VXFw
"""

!pip install transformers torch gradio

import torch

if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu")
    print("Using CPU")

from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration

# Load the BlenderBot 3B model
model_name = "facebook/blenderbot-3B"
tokenizer = BlenderbotTokenizer.from_pretrained(model_name)
model = BlenderbotForConditionalGeneration.from_pretrained(model_name)
model.to(device)

# Chat loop
def chat_with_bot(user_input):
    inputs = tokenizer([user_input], return_tensors="pt").to(device)
    reply_ids = model.generate(**inputs)
    reply_text = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]
    return reply_text

# Example usage
print(chat_with_bot("Hello, how are you?"))

import gradio as gr

def chat_with_blenderbot(user_input):
    reply_text = chat_with_bot(user_input)
    return reply_text
# UI layout using Blocks (vertical alignment)
with gr.Blocks() as demo:
    gr.Markdown("## BlenderBot Chatbot")

    with gr.Column():
        user_input = gr.Textbox(label="Your Message", placeholder="Type your message here...")
        output = gr.Textbox(label="Bot's Reply", interactive=False)

    submit_btn = gr.Button("Send")

    submit_btn.click(fn=chat_with_blenderbot, inputs=user_input, outputs=output)

demo.launch()