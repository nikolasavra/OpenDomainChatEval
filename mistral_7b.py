# -*- coding: utf-8 -*-
"""Mistral-7B.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wZG6N9hH-AjxrhZxLTshEWkrsGvig3Fa
"""

import pandas as pd
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

!huggingface-cli login

# Load Mistral 7B model and tokenizer
model_name = "mistralai/Mistral-7B-v0.1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Move model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Load the CSV file
dataset_path = "chit_chat.csv"
df = pd.read_csv(dataset_path)

# Apply the function to extract and clean first user prompts
user_prompts = df['content']

user_prompts = user_prompts.tolist()

def limit_to_two_sentences(text):
    sentences = re.split(r'(?<=[.!?]) +', text.strip())
    return ' '.join(sentences[:2])

def generate_response_mistral(prompt, max_new_tokens=100):
    try:
        if pd.isna(prompt) or not isinstance(prompt, str) or len(prompt.strip()) == 0:
            return "Invalid or empty prompt"

        # Add instruction to prompt
        full_prompt = f"{prompt.strip()}\nRespond in one or two short sentences."

        print(f"Processing prompt: {prompt[:100]}...")

        input_ids = tokenizer(full_prompt, return_tensors="pt", truncation=True, max_length=256).to(device)

        with torch.no_grad():
            outputs = model.generate(
                input_ids['input_ids'],
                attention_mask=input_ids['attention_mask'],
                max_new_tokens=max_new_tokens,
                num_return_sequences=1,
                temperature=0.7,
                top_k=50,
                top_p=0.9,
                repetition_penalty=1.1,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        input_length = input_ids['input_ids'].shape[1]
        generated_tokens = outputs[0][input_length:]
        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)

        short_response = limit_to_two_sentences(response)
        print(f"Generated response: {short_response[:100]}...")
        return short_response.strip()

    except Exception as e:
        print(f"Error generating response: {str(e)}")
        return f"Error: {str(e)}"

# Process all prompts
mistral_responses = []

for idx, prompt in enumerate(user_prompts):
    response = generate_response_mistral(prompt)
    mistral_responses.append(response)

    # Print progress every 100 prompts
    if (idx + 1) % 100 == 0:
        print(f"Processed {idx + 1}/{len(user_prompts)} prompts")

    # Print first 5 for verification
    if idx < 5:
        print(f"Prompt {idx + 1}: {prompt}")
        print(f"Response {idx + 1}: {response}")
        print("-" * 50)

# Create final DataFrame with all results
results_df = pd.DataFrame({
    'Prompt': user_prompts.tolist(),
    'Response': mistral_responses
})

# Save all results to a single CSV
output_filename = "mistral_responses_final.csv"
results_df.to_csv(output_filename, index=False)

print(f"\nCompleted! Saved {len(results_df)} responses to {output_filename}")
print(f"Sample of final results:\n{results_df.head()}")

# Convert the results to a DataFrame for easier analysis and save to CSV
df_responses = pd.DataFrame(results_df)
df_responses.to_csv("mistral_responses.csv", index=False)