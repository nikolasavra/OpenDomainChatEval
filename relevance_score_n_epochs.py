# -*- coding: utf-8 -*-
"""Relevance_score_N_epochs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jObnOIDY_KiERs695d7FkzdJpdRe5v0Y
"""

pip install pandas

import torch

if torch.cuda.is_available():
    device = torch.device("cuda")
    print(f"Using GPU: {torch.cuda.get_device_name(0)}")
else:
    device = torch.device("cpu")
    print("Using CPU")

import json
import pandas as pd
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

# Step 6: Define the model, optimizer, and loss function
# Load pre-trained model and tokenizer
model = DistilBertForSequenceClassification.from_pretrained('distilbert_12_epochs', num_labels=3)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Convert JSON data into a DataFrame for easier manipulation
df_mistral = pd.read_csv('mistral_responses.csv')

# Check the first few rows to ensure data is loaded correctly
print(df_mistral.head())

#ONLY FOR GPT-NEO RESPONSES

import pandas as pd
import re

def remove_first_sentence(text):
    # Find the first sentence-ending punctuation (., !, or ?)
    match = re.search(r'[.!?]', text)
    if match:
        # Return everything after the punctuation mark
        return text[match.end():].lstrip()  # lstrip to remove leading space
    return ''  # If no punctuation found, return empty string

# Apply to your DataFrame
df_gptneo['Response'] = df_gptneo['Response'].apply(remove_first_sentence)

import torch.nn.functional as F
# Function to tokenize the prompt and response, and evaluate using the MDD-Eval teacher model
def tokenize_and_evaluate(prompt, response, model, tokenizer, device):
    # Convert prompt and response to strings to handle potential non-string types
    prompt = str(prompt)
    response = str(response)

    # Tokenize the prompt (dialogue) and response
    inputs = tokenizer(prompt, response, truncation=True, padding='max_length', return_tensors="pt").to(device)

    # Forward pass through the MDD-Eval model (teacher model)
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

    # Get the predicted label (0: original, 1: adversarial, 2: random)
    predicted_label = torch.argmax(logits, dim=1).item()

    # Calculate relevance score (probability of "original" class)
    probabilities = F.softmax(logits, dim=1)  # Apply softmax to get probabilities
    relevance_score = probabilities[0, 0].item()  # Probability for the "original" class (index 0)

    return predicted_label, relevance_score

# List to store evaluation results
evaluation_results = []

# Loop through each prompt-response pair and evaluate using the MDD-Eval model
for index, row in df_mistral.iterrows():
    prompt = row['Prompt']
    response = row['Response']

    # Get the evaluation score and relevance score
    evaluation_label, relevance_score = tokenize_and_evaluate(prompt, response, model, tokenizer, device)

    # Store the result
    evaluation_results.append({
        'prompt': prompt,
        'response': response,
        'evaluation_label': evaluation_label,  # 0: original, 1: adversarial, 2: random
        'relevance_score': relevance_score     # Probability of the "original" class
    })

# Optionally, convert results to a DataFrame for easier analysis
df_mistral_results = pd.DataFrame(evaluation_results)
print(df_mistral_results.head())

# Analyze the evaluation results
original_count = df_mistral_results[df_mistral_results['evaluation_label'] == 0].shape[0]
adversarial_count = df_mistral_results[df_mistral_results['evaluation_label'] == 1].shape[0]
random_count = df_mistral_results[df_mistral_results['evaluation_label'] == 2].shape[0]

total_responses = len(df_mistral_results)

# Print summary of evaluation
print(f"Total responses evaluated: {total_responses}")
print(f"Original (good) responses: {original_count} ({(original_count/total_responses)*100:.2f}%)")
print(f"Adversarial responses: {adversarial_count} ({(adversarial_count/total_responses)*100:.2f}%)")
print(f"Random responses: {random_count} ({(random_count/total_responses)*100:.2f}%)")
print(f"Average relevance score: {df_mistral_results['relevance_score'].mean():.4f}")

df_mistral_results.to_csv('Mistral_relevance_scores_12.csv', index = False)

